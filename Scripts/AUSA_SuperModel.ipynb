{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras\n",
        "!pip install spektral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pUbXYW6lwu0",
        "outputId": "979a673f-df65-4c51-d7a7-eea4dfbc3ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting keras>=3.2.0 (from scikeras)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.4.2 (from scikeras)\n",
            "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
            "Collecting namex (from keras>=3.2.0->scikeras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.9.0)\n",
            "Collecting optree (from keras>=3.2.0->scikeras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Installing collected packages: namex, optree, scikit-learn, keras, scikeras\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.3.3 namex-0.0.8 optree-0.11.0 scikeras-0.13.0 scikit-learn-1.5.0\n",
            "Collecting spektral\n",
            "  Downloading spektral-1.3.1-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from spektral) (1.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from spektral) (4.9.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from spektral) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from spektral) (2.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from spektral) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from spektral) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spektral) (4.66.4)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from spektral) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (4.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow>=2.2.0->spektral)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2024.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->spektral) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.2.0->spektral) (3.2.2)\n",
            "Installing collected packages: keras, spektral\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.3.3\n",
            "    Uninstalling keras-3.3.3:\n",
            "      Successfully uninstalled keras-3.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikeras 0.13.0 requires keras>=3.2.0, but you have keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 spektral-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6W9_4DdlYqm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, GRU, SimpleRNN, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.layers import Input, Attention\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow_probability as tfp\n",
        "import networkx as nx\n",
        "from spektral.layers import GCNConv\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/sample_data/output.csv\")\n",
        "\n",
        "# Preprocess the dataset\n",
        "data = data.drop(columns=[\"Name\"])\n",
        "data_encoded = pd.get_dummies(data, columns=[\"Initial Continent\", \"Initial Climate\", \"Final Continent\", \"Final Climate\"])\n",
        "\n",
        "X = data_encoded.drop(columns=[\"Impact\"])\n",
        "y = data_encoded[[\"Impact\"]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ts_zqZ7GmmO6",
        "outputId": "9dee2c60-b32f-4de6-ee7b-e9c1a9d5fdd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Skin Tone Initial Continent Initial Climate  Initial Radiations  \\\n",
              "0          1              Asia            Cold                   1   \n",
              "1          2              Asia            Cold                   1   \n",
              "2          3              Asia            Cold                   1   \n",
              "3          4              Asia            Cold                   1   \n",
              "4          5              Asia            Cold                   1   \n",
              "\n",
              "   Initial Oxygen  Initial Nitrogen Final Continent Final Climate  \\\n",
              "0            0.79               0.2          Europe          Cold   \n",
              "1            0.79               0.2          Europe          Cold   \n",
              "2            0.79               0.2          Europe          Cold   \n",
              "3            0.79               0.2          Europe     Temperate   \n",
              "4            0.79               0.2          Europe     Temperate   \n",
              "\n",
              "   Final Radiations  Final Oxygen  Final Nitrogen  Duration  Impact  \n",
              "0                 1          0.79             0.2        10       0  \n",
              "1                 1          0.79             0.2         6       0  \n",
              "2                 1          0.79             0.2         9       0  \n",
              "3                 1          0.79             0.2         3       0  \n",
              "4                 1          0.79             0.2         5       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd9f74bf-d60a-400a-9018-53bd22ff38f7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Skin Tone</th>\n",
              "      <th>Initial Continent</th>\n",
              "      <th>Initial Climate</th>\n",
              "      <th>Initial Radiations</th>\n",
              "      <th>Initial Oxygen</th>\n",
              "      <th>Initial Nitrogen</th>\n",
              "      <th>Final Continent</th>\n",
              "      <th>Final Climate</th>\n",
              "      <th>Final Radiations</th>\n",
              "      <th>Final Oxygen</th>\n",
              "      <th>Final Nitrogen</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Asia</td>\n",
              "      <td>Cold</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Europe</td>\n",
              "      <td>Cold</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Asia</td>\n",
              "      <td>Cold</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Europe</td>\n",
              "      <td>Cold</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Asia</td>\n",
              "      <td>Cold</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Europe</td>\n",
              "      <td>Cold</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Asia</td>\n",
              "      <td>Cold</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Europe</td>\n",
              "      <td>Temperate</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Asia</td>\n",
              "      <td>Cold</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Europe</td>\n",
              "      <td>Temperate</td>\n",
              "      <td>1</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd9f74bf-d60a-400a-9018-53bd22ff38f7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fd9f74bf-d60a-400a-9018-53bd22ff38f7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fd9f74bf-d60a-400a-9018-53bd22ff38f7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6fbd9306-950f-4b8c-9e51-f2bdd05f6ba1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6fbd9306-950f-4b8c-9e51-f2bdd05f6ba1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6fbd9306-950f-4b8c-9e51-f2bdd05f6ba1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Skin Tone\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Continent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Asia\",\n          \"Africa\",\n          \"Australia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Climate\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Temperate\",\n          \"Tropical\",\n          \"Cold\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Radiations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Oxygen\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.221557105636704e-16,\n        \"min\": 0.79,\n        \"max\": 0.79,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.79\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Initial Nitrogen\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.77694638204588e-17,\n        \"min\": 0.2,\n        \"max\": 0.2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Continent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Europe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Climate\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Temperate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Radiations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Oxygen\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.221557105636704e-16,\n        \"min\": 0.79,\n        \"max\": 0.79,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.79\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Nitrogen\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.77694638204588e-17,\n        \"min\": 0.2,\n        \"max\": 0.2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Impact\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": -3,\n        \"max\": 2,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression(X_train, y_train, X_test):\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test), None\n",
        "\n",
        "def polynomial_regression(X_train, y_train, X_test, degree=2):\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    X_test_poly = poly.transform(X_test)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    return model.predict(X_test_poly), None\n",
        "\n",
        "def svr(X_train, y_train, X_test):\n",
        "    model = SVR(kernel='rbf')\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "    return model.predict(X_test), None\n",
        "\n",
        "def decision_tree(X_train, y_train, X_test):\n",
        "    model = DecisionTreeRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test), None\n",
        "\n",
        "def random_forest(X_train, y_train, X_test):\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "    return model.predict(X_test), None  # Return predictions and None for history\n",
        "\n",
        "def gradient_boosting(X_train, y_train, X_test):\n",
        "    model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "    return model.predict(X_test), None\n",
        "\n",
        "def xgboost(X_train, y_train, X_test):\n",
        "    model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "    return model.predict(X_test), None\n",
        "\n",
        "def adaboost(X_train, y_train, X_test):\n",
        "    model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "    return model.predict(X_test), None"
      ],
      "metadata": {
        "id": "tqAnLruznO6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm(X_train, y_train, X_test):\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(1, X_train_scaled.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=1))  # Output layer with 1 neuron for Impact\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test), epochs=100, batch_size=32, verbose=1)\n",
        "    return model.predict(X_test_reshaped), history\n",
        "\n",
        "\n",
        "def nn(X_train, y_train, X_test):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)\n",
        "    return model.predict(X_test), history\n",
        "\n",
        "\n",
        "\n",
        "def build_cnn():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_scaled.shape[1], 1)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def rnn(X_train, y_train, X_test):\n",
        "    # Reshape the input data to be 3D: (samples, timesteps, features)\n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "    X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "    model = Sequential([\n",
        "        SimpleRNN(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test), epochs=100, batch_size=32, verbose=1)\n",
        "    return model.predict(X_test_reshaped), history\n",
        "\n",
        "\n",
        "\n",
        "def gru(X_train, y_train, X_test):\n",
        "    print(\"X_train_scaled shape:\", X_train.shape)\n",
        "    print(\"X_test_scaled shape:\", X_test.shape)\n",
        "    # Build the GRU model\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=50, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=1))  # Output layer with 1 neuron for Impact\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_reshaped, y_train, epochs=300, batch_size=32, validation_data=(X_test_reshaped, y_test), verbose=1)\n",
        "    return model.predict(X_test), history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_autoencoder():\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    encoding_dim = 32\n",
        "\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "def autoencoder(X_train, y_train, X_test):\n",
        "    autoencoder = build_autoencoder()\n",
        "    history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test, X_test))\n",
        "    encoder = Model(autoencoder.input, autoencoder.layers[-2].output)\n",
        "    X_train_encoded = encoder.predict(X_train)\n",
        "    X_test_encoded = encoder.predict(X_test)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_encoded, y_train)\n",
        "    return model.predict(X_test_encoded), history\n",
        "\n",
        "\n",
        "def build_gnn():\n",
        "    input_shape = X_train_scaled.shape[1]\n",
        "    X_in = Input(shape=(input_shape,))\n",
        "    A_in = Input(shape=(None,))\n",
        "    X_1 = GCNConv(32, activation='relu')([X_in, A_in])\n",
        "    X_2 = GCNConv(1)([X_1, A_in])\n",
        "    model = Model(inputs=[X_in, A_in], outputs=X_2)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "\n",
        "def gnn(X_train, y_train, X_test):\n",
        "    A = np.eye(X_train.shape[1])\n",
        "    model = build_gnn()\n",
        "    history = model.fit([X_train_scaled, A], y_train, epochs=100, batch_size=32, verbose=1, validation_data=([X_test_scaled, A], y_test))\n",
        "    return model.predict([X_test_scaled, A]), history\n",
        "\n",
        "\n",
        "def build_attention():\n",
        "    input_shape = X_train_scaled.shape[1]\n",
        "    input_layer = Input(shape=(input_shape,))\n",
        "    attention_data = Attention()([input_layer, input_layer])\n",
        "    dense_layer = Dense(64, activation='relu')(attention_data)\n",
        "    output_layer = Dense(1)(dense_layer)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "\n",
        "def attention(X_train, y_train, X_test):\n",
        "    model = KerasRegressor(build_fn=build_attention, epochs=100, batch_size=32, verbose=1)\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
        "    return model.predict(X_test), history"
      ],
      "metadata": {
        "id": "hyXlGauZntGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save results incrementally\n",
        "def save_results(results):\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(\"/content/sample_data/model_results.csv\", index=False)\n",
        "    print(\"Results saved to model_results.csv\")"
      ],
      "metadata": {
        "id": "t3Pk95dworyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each model and save results incrementally\n",
        "results = []\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': linear_regression,\n",
        "    'Polynomial Regression': lambda X_train, y_train, X_test: polynomial_regression(X_train, y_train, X_test, degree=2),\n",
        "    'Support Vector Regression': svr,\n",
        "    'Decision Tree': decision_tree,\n",
        "    'Random Forest': random_forest,\n",
        "    'Gradient Boosting': gradient_boosting,\n",
        "    'XGBoost': xgboost,\n",
        "    'AdaBoost': adaboost,\n",
        "    'LSTM': lstm,\n",
        "    'Neural Network': nn,\n",
        "    'Recurrent Neural Network': rnn,\n",
        "    'GRU': gru,\n",
        "    'Autoencoder': autoencoder,\n",
        "    'Graph Neural Network': gnn,\n",
        "    'Attention Mechanism': attention\n",
        "}\n",
        "\n",
        "for name, model_func in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    start_time = time.time()\n",
        "    predictions, history = model_func(X_train_scaled, y_train, X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    end_time = time.time()\n",
        "    result = {'Model': name, 'Test Loss (MSE)': mse, 'Training Time (s)': end_time - start_time}\n",
        "    if history:\n",
        "        result['Training Loss'] = history.history['loss'][-1]\n",
        "        result['Validation Loss'] = history.history['val_loss'][-1]\n",
        "    results.append(result)\n",
        "    save_results(results)\n",
        "\n",
        "print(\"All models have been trained and results saved incrementally to model_results.csv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxDtQlhkoxZl",
        "outputId": "987694a8-3e40-4f49-f49c-7cb718d85474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Linear Regression...\n",
            "Results saved to model_results.csv\n",
            "Training Polynomial Regression...\n",
            "Results saved to model_results.csv\n",
            "Training Support Vector Regression...\n",
            "Results saved to model_results.csv\n",
            "Training Decision Tree...\n",
            "Results saved to model_results.csv\n",
            "Training Random Forest...\n",
            "Results saved to model_results.csv\n",
            "Training Gradient Boosting...\n",
            "Results saved to model_results.csv\n",
            "Training XGBoost...\n",
            "Results saved to model_results.csv\n",
            "Training AdaBoost...\n",
            "Results saved to model_results.csv\n",
            "Training LSTM...\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - 15s 59ms/step - loss: 1.8545 - val_loss: 1.8488\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 1.4358 - val_loss: 1.0712\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6417 - val_loss: 0.5747\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3996 - val_loss: 0.4626\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.3388 - val_loss: 0.4018\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2974 - val_loss: 0.3632\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2718 - val_loss: 0.3328\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2435 - val_loss: 0.3089\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2257 - val_loss: 0.2892\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2144 - val_loss: 0.2696\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2077 - val_loss: 0.2650\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1857 - val_loss: 0.2523\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1748 - val_loss: 0.2384\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1608 - val_loss: 0.2347\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1622 - val_loss: 0.2322\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1753 - val_loss: 0.2212\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1635 - val_loss: 0.2104\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1537 - val_loss: 0.2204\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1541 - val_loss: 0.2050\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1358 - val_loss: 0.2104\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1452 - val_loss: 0.2165\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1398 - val_loss: 0.2028\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1432 - val_loss: 0.2042\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1389 - val_loss: 0.1998\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1336 - val_loss: 0.2002\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1372 - val_loss: 0.1980\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1250 - val_loss: 0.1854\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1285 - val_loss: 0.1976\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1276 - val_loss: 0.1968\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1202 - val_loss: 0.1898\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1219 - val_loss: 0.1935\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1202 - val_loss: 0.1914\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1136 - val_loss: 0.1809\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1206 - val_loss: 0.1834\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1058 - val_loss: 0.1998\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1083 - val_loss: 0.1933\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1056 - val_loss: 0.1870\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1200 - val_loss: 0.1829\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1078 - val_loss: 0.1783\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1129 - val_loss: 0.1809\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1047 - val_loss: 0.1725\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1039 - val_loss: 0.1760\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1071 - val_loss: 0.1709\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0950 - val_loss: 0.1697\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1039 - val_loss: 0.1747\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0985 - val_loss: 0.1710\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0965 - val_loss: 0.1726\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.0925 - val_loss: 0.1680\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0958 - val_loss: 0.1670\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0953 - val_loss: 0.1695\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0951 - val_loss: 0.1645\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.1032 - val_loss: 0.1673\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0933 - val_loss: 0.1571\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0988 - val_loss: 0.1660\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0983 - val_loss: 0.1545\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0930 - val_loss: 0.1561\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0939 - val_loss: 0.1571\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0915 - val_loss: 0.1582\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0886 - val_loss: 0.1581\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0990 - val_loss: 0.1619\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0914 - val_loss: 0.1558\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0912 - val_loss: 0.1582\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0896 - val_loss: 0.1533\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0828 - val_loss: 0.1627\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0880 - val_loss: 0.1538\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.0910 - val_loss: 0.1524\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0897 - val_loss: 0.1549\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0805 - val_loss: 0.1449\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0922 - val_loss: 0.1554\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0802 - val_loss: 0.1573\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0921 - val_loss: 0.1562\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0845 - val_loss: 0.1530\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0803 - val_loss: 0.1474\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0807 - val_loss: 0.1478\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0838 - val_loss: 0.1448\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0822 - val_loss: 0.1461\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0798 - val_loss: 0.1500\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0789 - val_loss: 0.1459\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0817 - val_loss: 0.1479\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0846 - val_loss: 0.1485\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0799 - val_loss: 0.1440\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0783 - val_loss: 0.1411\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0794 - val_loss: 0.1455\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0840 - val_loss: 0.1466\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0790 - val_loss: 0.1477\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0812 - val_loss: 0.1450\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0753 - val_loss: 0.1511\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0761 - val_loss: 0.1454\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0794 - val_loss: 0.1426\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0740 - val_loss: 0.1431\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0750 - val_loss: 0.1414\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0745 - val_loss: 0.1393\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0697 - val_loss: 0.1331\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0775 - val_loss: 0.1388\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0760 - val_loss: 0.1315\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0757 - val_loss: 0.1375\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0683 - val_loss: 0.1360\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0783 - val_loss: 0.1427\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0694 - val_loss: 0.1370\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0755 - val_loss: 0.1393\n",
            "7/7 [==============================] - 1s 3ms/step\n",
            "Results saved to model_results.csv\n",
            "All models have been trained and results saved incrementally to model_results.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, Dropout, GRU, SimpleRNN, Bidirectional, Input, Dot, Activation, Reshape\n",
        "from keras.optimizers import Adam\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "# Add this import at the top of your 'ipython-input-1-4efe37c45b5f' file\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(data, target_column, categorical_columns):\n",
        "    data = data.drop(columns=[\"Name\"])  # Drop unnecessary columns like Name\n",
        "    data_encoded = pd.get_dummies(data, columns=categorical_columns)\n",
        "    X = data_encoded.drop(columns=[target_column])\n",
        "    y = data_encoded[[target_column]]\n",
        "    return X, y\n",
        "\n",
        "# Splitting and scaling function\n",
        "def split_and_scale(X, y, test_size=0.2, random_state=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "# Machine Learning Models\n",
        "def linear_regression_model(X_train, X_test, y_train, y_test):\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Linear Regression MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def svr_model(X_train, X_test, y_train, y_test):\n",
        "    model = SVR()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"SVR MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def decision_tree_model(X_train, X_test, y_train, y_test):\n",
        "    model = DecisionTreeRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Decision Tree Regressor MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def random_forest_model(X_train, X_test, y_train, y_test):\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Random Forest Regressor MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def xgboost_model(X_train, X_test, y_train, y_test):\n",
        "    model = xgb.XGBRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"XGBoost Regressor MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def adaboost_model(X_train, X_test, y_train, y_test):\n",
        "    model = AdaBoostRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"AdaBoost Regressor MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def gradient_boosting_model(X_train, X_test, y_train, y_test):\n",
        "    model = GradientBoostingRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Gradient Boosting Regressor MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "# Deep Learning Models\n",
        "def lstm_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"LSTM Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def bi_lstm_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=50, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Bidirectional(LSTM(units=50)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Bi-LSTM Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def gru_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GRU(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"GRU Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def rnn_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"RNN Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def attention_model(X_train, X_test, y_train, y_test):\n",
        "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    lstm_out = LSTM(50, return_sequences=True)(input_layer)\n",
        "    attention_score = Dense(1, activation='softmax')(lstm_out)\n",
        "    attention_mul = Dot(axes=1)([attention_score, lstm_out])\n",
        "    attention_output = Reshape((X_train.shape[2],))(attention_mul)\n",
        "    output_layer = Dense(1)(attention_output)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Attention Model MSE: {mse}\")\n",
        "    return model, predictions\n"
      ],
      "metadata": {
        "id": "N9ST2qtHsdUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = pd.read_csv(\"/content/sample_data/output.csv\")\n",
        "\n",
        "# Preprocess data\n",
        "categorical_columns = [\"Initial Continent\", \"Initial Climate\", \"Final Continent\", \"Final Climate\"]\n",
        "X, y = preprocess_data(data, target_column=\"Impact\", categorical_columns=categorical_columns)\n",
        "\n",
        "# Split and scale data\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = split_and_scale(X, y)\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {\n",
        "    \"Linear Regression\": linear_regression_model,\n",
        "    \"SVR\": svr_model,\n",
        "    \"Decision Tree\": decision_tree_model,\n",
        "    \"Random Forest\": random_forest_model,\n",
        "    \"XGBoost\": xgboost_model,\n",
        "    \"AdaBoost\": adaboost_model,\n",
        "    \"Gradient Boosting\": gradient_boosting_model\n",
        "}\n",
        "\n",
        "# Continue with the model training and evaluation\n",
        "for model_name, model_func in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model, predictions = model_func(X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "    print(f\"{model_name} training complete.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "uJ5h1vsTsnGx",
        "outputId": "03942152-ee36-421b-bb23-ae89c4b04ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Linear Regression...\n",
            "Linear Regression MSE: 0.4897251729873994\n",
            "Linear Regression training complete.\n",
            "\n",
            "Training SVR...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "<ipython-input-7-edd4b26f5247>:59: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  model.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVR MSE: 0.25172846899649765\n",
            "SVR training complete.\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Regressor MSE: 0.02\n",
            "Decision Tree training complete.\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Regressor MSE: 0.0148815\n",
            "Random Forest training complete.\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost Regressor MSE: 0.0063367193039440715\n",
            "XGBoost training complete.\n",
            "\n",
            "Training AdaBoost...\n",
            "AdaBoost Regressor MSE: 0.3113021773200784\n",
            "AdaBoost training complete.\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Regressor MSE: 0.10948036012641058\n",
            "Gradient Boosting training complete.\n",
            "\n",
            "Training LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4efe37c45b5f>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_name} training complete.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-edd4b26f5247>\u001b[0m in \u001b[0;36mlstm_model\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, Dropout, GRU, SimpleRNN, Bidirectional, Input, Dot, Activation, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Ensure input shape is correctly specified for each model\n",
        "def lstm_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"LSTM Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def bi_lstm_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=50, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Bidirectional(LSTM(units=50)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Bi-LSTM Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def gru_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GRU(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"GRU Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def rnn_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"RNN Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = pd.read_csv(\"/content/sample_data/output.csv\")\n",
        "categorical_columns = [\"Initial Continent\", \"Initial Climate\", \"Final Continent\", \"Final Climate\"]\n",
        "X, y = preprocess_data(data, target_column=\"Impact\", categorical_columns=categorical_columns)\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = split_and_scale(X, y)\n",
        "\n",
        "# Train and evaluate DL models\n",
        "dl_models = {\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"Bi-LSTM\": bi_lstm_model,\n",
        "    \"GRU\": gru_model,\n",
        "    \"RNN\": rnn_model\n",
        "}\n",
        "\n",
        "for model_name, model_func in dl_models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model, predictions = model_func(X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "    print(f\"{model_name} training complete.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "zRxmnmPXtpUX",
        "outputId": "e494acd5-8525-42df-d74c-f8b712b9f810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3d281a3506d6>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/output.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mcategorical_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Initial Continent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Initial Climate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Final Continent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Final Climate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Impact\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_and_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, Dropout, GRU, SimpleRNN, Bidirectional, Input, Dot, Activation, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "unit = 50\n",
        "\n",
        "# Ensure input shape is correctly specified for each model\n",
        "def lstm_model(X_train, X_test, y_train, y_test, unit, batch, epochs):\n",
        "    # Reshape data for LSTM [samples, time steps, features]\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=unit, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=unit))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch, validation_data=(X_test_reshaped, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test_reshaped)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"LSTM Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def bi_lstm_model(X_train, X_test, y_train, y_test, unit, batch, epochs):\n",
        "    # Reshape data for Bi-LSTM [samples, time steps, features]\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=unit, return_sequences=True), input_shape=(1, X_train.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Bidirectional(LSTM(units=unit)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch, validation_data=(X_test_reshaped, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test_reshaped)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Bi-LSTM Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def gru_model(X_train, X_test, y_train, y_test, unit, batch, epochs):\n",
        "    # Reshape data for GRU [samples, time steps, features]\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=unit, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GRU(units=unit))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch, validation_data=(X_test_reshaped, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test_reshaped)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"GRU Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "def rnn_model(X_train, X_test, y_train, y_test, unit, batch, epochs):\n",
        "    # Reshape data for RNN [samples, time steps, features]\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=unit, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(units=unit))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch, validation_data=(X_test_reshaped, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test_reshaped)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"RNN Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = pd.read_csv(\"/content/sample_data/output.csv\")\n",
        "categorical_columns = [\"Initial Continent\", \"Initial Climate\", \"Final Continent\", \"Final Climate\"]\n",
        "X, y = preprocess_data(data, target_column=\"Impact\", categorical_columns=categorical_columns)\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = split_and_scale(X, y)\n",
        "\n",
        "# Train and evaluate DL models and save results to CSV\n",
        "dl_models = {\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"Bi-LSTM\": bi_lstm_model,\n",
        "    \"GRU\": gru_model,\n",
        "    \"RNN\": rnn_model\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model_func in dl_models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model, predictions = model_func(X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"{model_name} training complete.\\n\")\n",
        "\n",
        "    # Collect results\n",
        "    result_df = pd.DataFrame({\n",
        "        'True_Values': y_test,\n",
        "        'Predictions': predictions.flatten(),\n",
        "        'Model': model_name,\n",
        "        'MSE': mse\n",
        "    })\n",
        "    results.append(result_df)\n",
        "\n",
        "# Combine all results into a single DataFrame\n",
        "final_results = pd.concat(results, ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "final_results.to_csv(\"/content/sample_data/model_predictions.csv\", index=False)\n",
        "print(\"Results saved to model_predictions.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NdGUh2PqatCj",
        "outputId": "bf8b2a39-b1d1-4ca0-e200-9c3438fe5257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-232733f501f8>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/output.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mcategorical_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Initial Continent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Initial Climate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Final Continent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Final Climate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Impact\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_and_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, Dropout, GRU, SimpleRNN, Bidirectional, Input\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_data(data, target_column, categorical_columns):\n",
        "    # Convert categorical columns to dummy/indicator variables\n",
        "    data = pd.get_dummies(data, columns=categorical_columns)\n",
        "\n",
        "    # Ensure all features are numeric and handle NaN values\n",
        "    for column in data.columns:\n",
        "        if data[column].dtype == object:\n",
        "            data[column] = pd.to_numeric(data[column], errors='coerce')\n",
        "\n",
        "    # Fill NaN values with the mean of each column\n",
        "    data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "    # Split the data into features and target\n",
        "    X = data.drop(columns=[target_column])\n",
        "    y = data[target_column]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Function to split and scale the data\n",
        "def split_and_scale(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "# Define the LSTM model\n",
        "def lstm_model(X_train, X_test, y_train, y_test, unit, batch, epochs):\n",
        "    # Reshape data for LSTM [samples, time steps, features]\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=unit, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=unit))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch, validation_data=(X_test_reshaped, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test_reshaped)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"LSTM Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "# Define the Bi-LSTM model\n",
        "def bi_lstm_model(X_train, X_test, y_train, y_test, unit, batch, epochs):\n",
        "    # Reshape data for Bi-LSTM [samples, time steps, features]\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=unit, return_sequences=True), input_shape=(1, X_train.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Bidirectional(LSTM(units=unit)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch, validation_data=(X_test_reshaped, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test_reshaped)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Bi-LSTM Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "# Define the GRU model\n",
        "def gru_model(X_train, X_test, y_train, y_test, unit, batch, epochs):\n",
        "    # Reshape data for GRU [samples, time steps, features]\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=unit, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GRU(units=unit))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch, validation_data=(X_test_reshaped, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test_reshaped)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"GRU Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "# Define the RNN model\n",
        "def rnn_model(X_train, X_test, y_train, y_test, unit, batch, epochs):\n",
        "    # Reshape data for RNN [samples, time steps, features]\n",
        "    X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=unit, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(units=unit))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch, validation_data=(X_test_reshaped, y_test), verbose=0)\n",
        "    predictions = model.predict(X_test_reshaped)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"RNN Model MSE: {mse}\")\n",
        "    return model, predictions\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = pd.read_csv(\"/content/sample_data/output.csv\")\n",
        "categorical_columns = [\"Initial Continent\", \"Initial Climate\", \"Final Continent\", \"Final Climate\"]\n",
        "X, y = preprocess_data(data, target_column=\"Impact\", categorical_columns=categorical_columns)\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = split_and_scale(X, y)\n",
        "\n",
        "# Train and evaluate deep learning models and save results to CSV\n",
        "dl_models = {\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"Bi-LSTM\": bi_lstm_model,\n",
        "    \"GRU\": gru_model,\n",
        "    \"RNN\": rnn_model\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model_func in dl_models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model, predictions = model_func(X_train_scaled, X_test_scaled, y_train, y_test, unit=50, batch=32, epochs=100)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"{model_name} training complete.\\n\")\n",
        "\n",
        "    # Collect results\n",
        "    result_df = pd.DataFrame({\n",
        "        'True_Values': y_test,\n",
        "        'Predictions': predictions.flatten(),\n",
        "        'Model': model_name,\n",
        "        'MSE': mse\n",
        "    })\n",
        "    results.append(result_df)\n",
        "\n",
        "# Combine all results into a single DataFrame\n",
        "final_results = pd.concat(results, ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "final_results.to_csv(\"/content/sample_data/model_predictions.csv\", index=False)\n",
        "print(\"Results saved to model_predictions.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6nz5LZpbOfR",
        "outputId": "9c37fbab-5393-4289-e5d3-c2948ded9f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1047: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1052: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/extmath.py:1072: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM...\n"
          ]
        }
      ]
    }
  ]
}